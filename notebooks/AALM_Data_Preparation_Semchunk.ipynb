{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AALM — Data Preparation with semchunk\n",
    "Build an Administrative Law–focused dataset from the Open Australian Legal Corpus using `semchunk`.\n",
    "\n",
    "This notebook filters the corpus for Administrative Law material by keywords (tribunals, judicial review concepts, FOI, etc.),\n",
    "chunks the texts with `semchunk`, and saves a `text`-only dataset for SFT/QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U datasets semchunk transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, itertools\n",
    "from typing import Dict, Any, List\n",
    "from datasets import load_dataset, Dataset\n",
    "import semchunk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "CORPUS_DATASET = os.environ.get('CORPUS_DATASET', 'isaacus/open-australian-legal-corpus')\n",
    "CORPUS_SPLIT = os.environ.get('CORPUS_SPLIT', 'corpus')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', 'data/aalm-adminlaw-semchunk')\n",
    "BASE_TOKENIZER = os.environ.get('BASE_TOKENIZER', 'openai/gpt-oss-20b')  # used for token counting\n",
    "CHUNK_SIZE = int(os.environ.get('CHUNK_SIZE', '1024'))\n",
    "OVERLAP = float(os.environ.get('OVERLAP', '0.2'))\n",
    "DOC_LIMIT = int(os.environ.get('DOC_LIMIT', '0'))  # 0 = no explicit cap; use for quick dry runs\n",
    "\n",
    "# Broad Administrative Law signal via keywords (case-insensitive)\n",
    "ADMIN_KEYWORDS = [\n",
    "    'administrative appeals tribunal', 'aat', 'administrative decisions tribunal',\n",
    "    'civil and administrative tribunal', 'ncat', 'vcat', 'qcat', 'acat',\n",
    "    'merits review', 'judicial review', 'procedural fairness', 'natural justice',\n",
    "    'jurisdictional error', 'wednesbury', 'unreasonableness',\n",
    "    'freedom of information', 'foi', 'ombudsman',\n",
    "    'delegate', 'delegated legislation', 'minister', 'review of decision',\n",
    "    'administrative arrangement'\n",
    "]\n",
    "KW = re.compile('|'.join(re.escape(k) for k in ADMIN_KEYWORDS), flags=re.I)\n",
    "\n",
    "def is_admin_law(record: Dict[str, Any]) -> bool:\n",
    "    citation = (record.get('citation') or '')\n",
    "    text = (record.get('text') or '')\n",
    "    return bool(KW.search(citation + '\\n' + text))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_TOKENIZER, use_fast=True)\n",
    "chunker = semchunk.chunkerify(tokenizer, chunk_size=min(CHUNK_SIZE, getattr(tokenizer, 'model_max_length', CHUNK_SIZE)))\n",
    "print('Using tokenizer:', BASE_TOKENIZER)\n",
    "print('Chunk size:', CHUNK_SIZE, 'Overlap:', OVERLAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and filter the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_dataset(CORPUS_DATASET, split=CORPUS_SPLIT, keep_in_memory=False)\n",
    "print('Total documents in split:', len(corpus))\n",
    "\n",
    "admin_docs = []\n",
    "count = 0\n",
    "for ex in corpus:\n",
    "    if not ex.get('text'):\n",
    "        continue\n",
    "    if is_admin_law(ex):\n",
    "        admin_docs.append(ex)\n",
    "        count += 1\n",
    "        if DOC_LIMIT and count >= DOC_LIMIT:\n",
    "            break\n",
    "print('Matched Administrative Law docs:', len(admin_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk texts with semchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts: List[str] = []\n",
    "citations: List[str] = []\n",
    "jurisdictions: List[str] = []\n",
    "types: List[str] = []\n",
    "urls: List[str] = []\n",
    "\n",
    "for ex in admin_docs:\n",
    "    chunks = chunker(ex['text'], overlap=OVERLAP)\n",
    "    n = len(chunks)\n",
    "    texts.extend(chunks)\n",
    "    citations.extend([ex.get('citation') or ''] * n)\n",
    "    jurisdictions.extend([ex.get('jurisdiction') or ''] * n)\n",
    "    types.extend([ex.get('type') or ''] * n)\n",
    "    urls.extend([ex.get('url') or ''] * n)\n",
    "\n",
    "print('Total chunks:', len(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Dataset.from_dict({\n",
    "    'text': texts,\n",
    "    'citation': citations,\n",
    "    'jurisdiction': jurisdictions,\n",
    "    'type': types,\n",
    "    'url': urls,\n",
    "})\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "out.save_to_disk(OUTPUT_DIR)\n",
    "print('Saved to', OUTPUT_DIR)\n",
    "out[:2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

