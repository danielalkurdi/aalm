{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AALM — QLoRA Fine-tune on openai/gpt-oss-20b\n",
    "Fine-tune GPT‑OSS‑20B on the Australian legal QA dataset using TRL + PEFT (QLoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U datasets transformers accelerate peft bitsandbytes trl evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "BASE_MODEL = os.environ.get('BASE_MODEL', 'openai/gpt-oss-20b')\n",
    "DATASET = os.environ.get('HF_DATASET', 'isaacus/open-australian-legal-qa')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', 'outputs/aalm-gpt-oss-20b-qlora')\n",
    "MAX_STEPS = int(os.environ.get('MAX_STEPS', '300'))  # start small to sanity-check\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1'))\n",
    "GRAD_ACCUM = int(os.environ.get('GRAD_ACCUM', '32'))\n",
    "MAX_SEQ_LENGTH = int(os.environ.get('MAX_SEQ_LENGTH', '2048'))\n",
    "LEARNING_RATE = float(os.environ.get('LR', '2e-4'))\n",
    "WARMUP_RATIO = float(os.environ.get('WARMUP_RATIO', '0.03'))\n",
    "SAVE_STEPS = int(os.environ.get('SAVE_STEPS', '100'))\n",
    "LOGGING_STEPS = int(os.environ.get('LOGGING_STEPS', '10'))\n",
    "SEED = int(os.environ.get('SEED', '42'))\n",
    "USE_BF16 = True  # recommended on Ampere+\n",
    "LOAD_IN_4BIT = True  # QLoRA\n",
    "GRAD_CHECKPOINTING = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET)\n",
    "display(ds)\n",
    "example = ds['train'][0]\n",
    "example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and chat template\n",
    "GPT‑OSS expects the repo's chat template (harmony). We will format samples using `apply_chat_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "has_chat = isinstance(getattr(tokenizer, 'chat_template', None), str)\n",
    "has_chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map dataset to formatted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    'You are AALM, the Australian Administrative Law Model. '\n",
    "    'Answer legal questions about Australian law and cases accurately and concisely. '\n",
    "    'Cite the source when appropriate and never fabricate citations.'\n",
    ")\n",
    "\n",
    "def format_sample(sample: Dict) -> str:\n",
    "    q = sample.get('question')\n",
    "    a = sample.get('answer')\n",
    "    if not q or not a:\n",
    "        t = sample.get('text')\n",
    "        if t:\n",
    "            return t\n",
    "        raise ValueError('Sample missing question/answer/text fields')\n",
    "    if has_chat and hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': q},\n",
    "            {'role': 'assistant', 'content': a},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return f'{SYSTEM_PROMPT}\n\nQuestion: {q}\nAnswer: {a}'\n",
    "\n",
    "def map_fn(examples):\n",
    "    first_key = next(iter(examples.keys()))\n",
    "    n = len(examples[first_key])\n",
    "    texts = []\n",
    "    for i in range(n):\n",
    "        texts.append(format_sample({\n",
    "            'question': examples.get('question', [None]*n)[i],\n",
    "            'answer': examples.get('answer', [None]*n)[i],\n",
    "            'text': examples.get('text', [None]*n)[i],\n",
    "        }))\n",
    "    return {'text': texts}\n",
    "\n",
    "train_ds = ds['train'].map(map_fn, batched=True, remove_columns=ds['train'].column_names)\n",
    "train_ds = train_ds.shuffle(seed=SEED)\n",
    "train_ds[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 4‑bit base and attach LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    ")\n",
    "dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=dtype,\n",
    "    device_map='auto',\n",
    ")\n",
    "model_type = getattr(getattr(model, 'config', None), 'model_type', '')\n",
    "model_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_lora_targets(model_type: str) -> List[str]:\n",
    "    mt = (model_type or '').lower()\n",
    "    if mt == 'gpt_oss':\n",
    "        return ['q_proj','k_proj','v_proj','o_proj']\n",
    "    if mt == 'gpt_neox':\n",
    "        return ['query_key_value','dense','dense_h_to_4h','dense_4h_to_h']\n",
    "    if mt in {'llama','mistral','mixtral'}:\n",
    "        return ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj']\n",
    "    return ['q_proj','k_proj','v_proj','o_proj']\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=64, lora_alpha=16, lora_dropout=0.05, bias='none', task_type='CAUSAL_LM',\n",
    "    target_modules=pick_lora_targets(model_type),\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=0.0,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    bf16=USE_BF16, fp16=not USE_BF16,\n",
    "    seed=SEED,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    gradient_checkpointing=GRAD_CHECKPOINTING,\n",
    "    do_eval=False,\n",
    "    report_to=['none'],\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model, tokenizer=tokenizer, train_dataset=train_ds, dataset_text_field='text', args=training_args\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print('Saved to', OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

