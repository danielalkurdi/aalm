{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AALM — QLoRA Fine-tune on openai/gpt-oss-20b\n",
    "Fine-tune GPT‑OSS‑20B using TRL + PEFT (QLoRA) on an Administrative Law dataset prepared with semchunk from the Open Australian Legal Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U datasets transformers accelerate peft bitsandbytes trl evaluate semchunk tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "BASE_MODEL = os.environ.get('BASE_MODEL', 'openai/gpt-oss-20b')\n",
    "# Prefer a locally prepared dataset built with `AALM_Data_Preparation_Semchunk.ipynb`\n",
    "DATASET_DIR = os.environ.get('DATASET_DIR', 'data/aalm-adminlaw-semchunk')\n",
    "# Fallback: build a small on-the-fly dataset from the Corpus if local dataset isn't available\n",
    "CORPUS_DATASET = os.environ.get('CORPUS_DATASET', 'isaacus/open-australian-legal-corpus')\n",
    "OUTPUT_DIR = os.environ.get('OUTPUT_DIR', 'outputs/aalm-gpt-oss-20b-qlora')\n",
    "MAX_STEPS = int(os.environ.get('MAX_STEPS', '300'))  # start small to sanity-check\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = int(os.environ.get('BATCH_SIZE', '1'))\n",
    "GRAD_ACCUM = int(os.environ.get('GRAD_ACCUM', '32'))\n",
    "MAX_SEQ_LENGTH = int(os.environ.get('MAX_SEQ_LENGTH', '2048'))\n",
    "LEARNING_RATE = float(os.environ.get('LR', '2e-4'))\n",
    "WARMUP_RATIO = float(os.environ.get('WARMUP_RATIO', '0.03'))\n",
    "SAVE_STEPS = int(os.environ.get('SAVE_STEPS', '100'))\n",
    "LOGGING_STEPS = int(os.environ.get('LOGGING_STEPS', '10'))\n",
    "SEED = int(os.environ.get('SEED', '42'))\n",
    "USE_BF16 = True  # recommended on Ampere+\n",
    "LOAD_IN_4BIT = True  # QLoRA\n",
    "GRAD_CHECKPOINTING = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect dataset\n",
    "This notebook expects a local semchunked dataset at `DATASET_DIR` created by the data prep notebook. If it isn't found, it will attempt to create a small temporary dataset by sampling and chunking from the Open Australian Legal Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, re\n",
    "import semchunk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_or_build_dataset(tokenizer_name: str):\n",
    "    # Try local prepared dataset first\n",
    "    try:\n",
    "        ds_local = load_from_disk(DATASET_DIR)\n",
    "        return {'train': ds_local}\n",
    "    except Exception as e:\n",
    "        print(f'Local dataset not found at {DATASET_DIR}:', e)\n",
    "        print('Falling back to small, on-the-fly chunking from the Corpus...')\n",
    "\n",
    "    # Build a tiny fallback dataset from the Corpus for sanity check\n",
    "    corpus = load_dataset(CORPUS_DATASET, split='corpus', keep_in_memory=False)\n",
    "\n",
    "    admin_keywords = [\n",
    "        'administrative appeals tribunal', 'aat', 'ncat', 'vcat', 'acat', 'qcat',\n",
    "        'civil and administrative tribunal', 'administrative decisions tribunal',\n",
    "        'procedural fairness', 'natural justice', 'judicial review',\n",
    "        'jurisdictional error', 'unreasonableness', 'wednesbury', 'foi',\n",
    "        'freedom of information', 'ombudsman', 'review of decision', 'minister',\n",
    "        'delegated legislation', 'administrative arrangement'\n",
    "    ]\n",
    "    kw = re.compile('|'.join(re.escape(k) for k in admin_keywords), flags=re.I)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
    "    chunker = semchunk.chunkerify(tokenizer, chunk_size=min(1024, getattr(tokenizer, 'model_max_length', 2048)))\n",
    "\n",
    "    texts = []\n",
    "    limit_docs = int(os.environ.get('FALLBACK_DOC_LIMIT', '200'))\n",
    "    taken = 0\n",
    "    for ex in corpus:\n",
    "        if ex.get('text') and kw.search((ex.get('citation') or '') + '\n' + ex['text']):\n",
    "            for ch in chunker(ex['text'], overlap=0.2):\n",
    "                texts.append(ch)\n",
    "            taken += 1\n",
    "            if taken >= limit_docs:\n",
    "                break\n",
    "\n",
    "    from datasets import Dataset\n",
    "    tiny = Dataset.from_dict({'text': texts})\n",
    "    return {'train': tiny}\n",
    "\n",
    "ds = load_or_build_dataset(BASE_MODEL)\n",
    "display(ds)\n",
    "example = ds['train'][0]\n",
    "example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and chat template\n",
    "GPT‑OSS expects the repo's chat template (harmony). We will format samples using `apply_chat_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "has_chat = isinstance(getattr(tokenizer, 'chat_template', None), str)\n",
    "has_chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map dataset to formatted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    'You are AALM, the Australian Administrative Law Model. '\n",
    "    'Answer legal questions about Australian law and cases accurately and concisely. '\n",
    "    'Cite the source when appropriate and never fabricate citations.'\n",
    ")\n",
    "\n",
    "def format_sample(sample: Dict) -> str:\n",
    "    q = sample.get('question')\n",
    "    a = sample.get('answer')\n",
    "    if not q or not a:\n",
    "        t = sample.get('text')\n",
    "        if t:\n",
    "            return t\n",
    "        raise ValueError('Sample missing question/answer/text fields')\n",
    "    if has_chat and hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': q},\n",
    "            {'role': 'assistant', 'content': a},\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return f'{SYSTEM_PROMPT}\n\nQuestion: {q}\nAnswer: {a}'\n",
    "\n",
    "def map_fn(examples):\n",
    "    first_key = next(iter(examples.keys()))\n",
    "    n = len(examples[first_key])\n",
    "    texts = []\n",
    "    for i in range(n):\n",
    "        texts.append(format_sample({\n",
    "            'question': examples.get('question', [None]*n)[i],\n",
    "            'answer': examples.get('answer', [None]*n)[i],\n",
    "            'text': examples.get('text', [None]*n)[i],\n",
    "        }))\n",
    "    return {'text': texts}\n",
    "\n",
    "train_ds = ds['train'].map(map_fn, batched=True, remove_columns=ds['train'].column_names)\n",
    "train_ds = train_ds.shuffle(seed=SEED)\n",
    "train_ds[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 4‑bit base and attach LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    ")\n",
    "dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=dtype,\n",
    "    device_map='auto',\n",
    ")\n",
    "model_type = getattr(getattr(model, 'config', None), 'model_type', '')\n",
    "model_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_lora_targets(model_type: str) -> List[str]:\n",
    "    mt = (model_type or '').lower()\n",
    "    if mt == 'gpt_oss':\n",
    "        return ['q_proj','k_proj','v_proj','o_proj']\n",
    "    if mt == 'gpt_neox':\n",
    "        return ['query_key_value','dense','dense_h_to_4h','dense_4h_to_h']\n",
    "    if mt in {'llama','mistral','mixtral'}:\n",
    "        return ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj']\n",
    "    return ['q_proj','k_proj','v_proj','o_proj']\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=64, lora_alpha=16, lora_dropout=0.05, bias='none', task_type='CAUSAL_LM',\n",
    "    target_modules=pick_lora_targets(model_type),\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=0.0,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    bf16=USE_BF16, fp16=not USE_BF16,\n",
    "    seed=SEED,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    gradient_checkpointing=GRAD_CHECKPOINTING,\n",
    "    do_eval=False,\n",
    "    report_to=['none'],\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model, tokenizer=tokenizer, train_dataset=train_ds, dataset_text_field='text', args=training_args\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print('Saved to', OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
