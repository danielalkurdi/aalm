{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AALM â€” Inference and Merge\n",
    "Use the trained LoRA adapter for inference, and optionally merge it into the base weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U transformers peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = 'openai/gpt-oss-20b'\n",
    "ADAPTER_DIR = 'outputs/aalm-gpt-oss-20b-qlora'  # set to your path\n",
    "USE_BF16 = True\n",
    "SYSTEM = 'You are AALM, the Australian Administrative Law Model.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=dtype, device_map='auto')\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "model.eval()\n",
    "has_chat = isinstance(getattr(tokenizer, 'chat_template', None), str)\n",
    "has_chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question: str, system: str = SYSTEM, max_new_tokens: int = 512):\n",
    "    if has_chat and hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system},\n",
    "            {'role': 'user', 'content': question},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        text = f'{system}\n\nQuestion: {question}\nAnswer:'\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, do_sample=True, temperature=0.7, top_p=0.9,\n",
    "                             max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate('In New South Wales, when is procedural fairness required in administrative decision-making?'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge adapter into base (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "OUTPUT_MERGED = 'outputs/aalm-gpt-oss-20b-merged'\n",
    "merged = model.merge_and_unload()\n",
    "merged.save_pretrained(OUTPUT_MERGED)\n",
    "tokenizer.save_pretrained(OUTPUT_MERGED)\n",
    "print('Merged model saved to', OUTPUT_MERGED)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

